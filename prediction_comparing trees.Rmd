---
title: "Prediction: Comparing Trees"
author: "Lena Kafka"
date: "1/9/2019"
output: html_document
---

## Data

The data comes from a university registrar's office. The code book for the variables are available in the file code-book.txt. Examine the variables and their definitions.

Upload the drop-out.csv data into R as a data frame. 

```{r}
D1 <- read.csv("drop-out.csv", header = TRUE)
```

The next step is to separate your data set into a training set and a test set. Randomly select 25% of the students to be the test data set and leave the remaining 75% for your training data set. (Hint: each row represents an answer, not a single student.)

```{r}
library(dplyr)

SAMPLE <- data.frame(sample(unique(D1$student_id), 511))
names(SAMPLE) <- c("student_id")

TRAIN1 <- inner_join(D1, SAMPLE, by = "student_id")
TEST1 <- anti_join(D1, SAMPLE, by = "student_id")


```

we will be predicting the student level variable "complete". 

```{r}
 #Out of 10 variables, we remove 2 irrelavant variables; student_id and course_id
```

Visualize the relationships between your chosen variables as a scatterplot matrix. image saved as a .pdf named scatterplot_matrix.pdf. 
```{r}
pairs(complete~.,data=TRAIN1,main = "main scatterplot matrix")
#nothing really interesting found.. maybe scatterplot matrix is more useful when comparing numeric variables 

```

## CART Trees

Construct a classification tree that predicts complete using the caret package.

```{r}
library(caret)

TRAIN2 <- TRAIN1[,c(2:10)] #Remove the student_id variable that we do not want to use in the model

#Define the control elements we would like to use
ctrl <- trainControl(method = "repeatedcv", #Tell caret to perform 10-fold cross validation
                repeats = 3, #Tell caret to repeat each fold three times
                classProbs = TRUE, #Calculate class probabilities for ROC calculation
                summaryFunction = twoClassSummary)

#Define the model
cartFit <- train(complete ~ ., #Define which variable to predict 
                data = TRAIN2, #Define the data set to train the model on
                trControl = ctrl, #Tell caret the control elements (EG - the cross validation)
                method = "rpart", #Define the model type
                metric = "ROC", #Tell caret to calculate the ROC curve
                preProc = c("center", "scale")) #Center and scale the data to minimize the 

#Check the results
cartFit
                
#Plot ROC against complexity 
plot(cartFit)

```


##evaluation

ROC was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.01179071. It is a relative successful model because the ROC is closer to 1 with CP being at a relative low level(0.01)

##interpretation
With the Complexity Parameter going up, the ROC is going down. So we need to prune the tree when CP is relatively small and ROC relatively large for the moedel.


##predictions

```{r}
TEST2 <- TEST1[,c(2:10)] #Remove the student_id variable that we do not want to use in the model

#Generate prediction using previously trained model
cartClasses <- predict(cartFit, newdata = TEST2)

#Generate model statistics
confusionMatrix(data = cartClasses, TEST2$complete)

#it's a pretty good prediciton for the accuracy being as large las 0.905
```


 
## C4.5-Type Trees

We now repeat the same prediction but using a different tree-based algorithm called [J48](). J48 is a Java implementation of the C4.5 decsion tree algorithm of [Quinlan (1993)](https://link.springer.com/article/10.1007%2FBF00993309). 


## IF YOU CANNOT MAKE WEKA WORK PLEASE SKIP TO LINE 154
Train the J48 model on the same training data and examine your results.
```{r}
ctrl <- trainControl(method = "repeatedcv",
                repeats = 3,
                classProbs = TRUE,
                summaryFunction = twoClassSummary)

j48Fit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "J48",
                metric = "ROC",
                preProc = c("center", "scale"))

j48Fit

plot(j48Fit)
```


Now test our new J48 model by predicting the test data and generating model fit statistics.

```{r}

j48Classes <- predict(j48Fit, newdata = TEST2)

confusionMatrix(data = j48Classes, TEST2$complete)

```

## Alternative to Weka 
Train a Conditional Inference Tree using the `party` package on the same training data and examine your results.
```{r}
ctrl <- trainControl(method = "repeatedcv",
                repeats = 3,
                classProbs = TRUE,
                summaryFunction = twoClassSummary)

ctreeFit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "ctree",
                metric = "ROC",
                preProc = c("center", "scale"))

ctreeFit

plot(ctreeFit)
```

Now test our new Conditional Inference model by predicting the test data and generating model fit statistics.

##The C50 Tree

There is an updated version of the C4.5 model called C5.0, it is implemented in the C50 package.  

Install the C50 package, train and then test the C5.0 model on the same data.

```{r}
c50Fit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "C5.0",
                metric = "ROC",
                preProc = c("center", "scale"))

c50Fit

plot(c50Fit)

c50Classes <- predict(c50Fit, newdata = TEST2)

confusionMatrix(data = c50Classes, TEST2$complete)
```


## Compare the models

caret allows us to compare all three models at once.

```{r}
resamps <- resamples(list(cart = cartFit, jfoureight = j48Fit, cfiveo = c50Fit))
summary(resamps)
```
